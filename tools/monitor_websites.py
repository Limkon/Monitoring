import os
import sys
import requests
import time
from datetime import datetime, timezone
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback # å¯¼å…¥ traceback æ¨¡å—
import math # å¯¼å…¥ math æ¨¡å—ç”¨äºå‘ä¸Šå–æ•´

# --- é…ç½®å¸¸é‡ ---
REQUEST_TIMEOUT = 10  # seconds
README_FILENAME = "README.md"
USER_AGENT = "WebsiteStatusMonitor/1.0 (+https://github.com/your_username/your_repo)" # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ä¿¡æ¯
MAX_WORKERS = 10 # å¹¶å‘æ£€æŸ¥çš„æœ€å¤§çº¿ç¨‹æ•°

def normalize_url(url):
    """æ£€æŸ¥ URL æ˜¯å¦åŒ…å«åè®®ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™æ·»åŠ  https://"""
    parsed_url = urlparse(url.strip())
    if not parsed_url.scheme:
        return "https://" + url.strip()
    return url.strip()

def looks_like_url(url_str):
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦çœ‹èµ·æ¥åƒ URL"""
    if not isinstance(url_str, str) or not url_str.strip():
        return False
    try:
        parsed_url = urlparse(url_str.strip())
        return bool(parsed_url.netloc and '.' in parsed_url.netloc) and \
               bool(parsed_url.scheme or parsed_url.path or parsed_url.netloc)
    except ValueError:
        return False

def check_website_status(url):
    """æ£€æŸ¥ç½‘ç«™çŠ¶æ€å¹¶è¿”å›ç»“æœå­—å…¸"""
    headers = {'User-Agent': USER_AGENT}
    result = {
        "url": url,
        "status_code": None,
        "response_time": "N/A",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "error": None,
        "status": "â“ æœªçŸ¥çŠ¶æ€" # é»˜è®¤çŠ¶æ€
    }
    try:
        start_time = time.time()
        response = requests.get(url, timeout=REQUEST_TIMEOUT, headers=headers, allow_redirects=True)
        end_time = time.time()
        response_time_ms = (end_time - start_time) * 1000

        result["status_code"] = response.status_code
        result["response_time"] = f"{response_time_ms:.2f} ms"
        if response.url != url:
            result["final_url"] = response.url

        if 200 <= response.status_code < 300:
            result["status"] = "âœ… æ­£å¸¸"
        elif 300 <= response.status_code < 400:
            result["status"] = f"â†ªï¸ é‡å®šå‘ ({response.status_code})"
        elif response.status_code == 404:
            result["status"] = "ğŸš« æœªæ‰¾åˆ° (404)"
        else:
            result["status"] = f"âš ï¸ å¼‚å¸¸ (çŠ¶æ€: {response.status_code})"

        final_url_info = f" | æœ€ç»ˆURL: {result['final_url']}" if "final_url" in result else ""
        print(f"{result['status']} - {url} | Code: {result['status_code']} | Time: {result['response_time']}{final_url_info}")
        return result

    except requests.Timeout:
        result["status"] = "âŒ å¤ªæ…¢ (è¶…æ—¶)"
        result["error"] = "è¯·æ±‚è¶…æ—¶"
    except requests.exceptions.SSLError as e_ssl:
        result["status"] = "âŒ SSLé”™è¯¯"
        result["error"] = f"SSLæ¡æ‰‹æˆ–è¯ä¹¦éªŒè¯å¤±è´¥: {str(e_ssl).splitlines()[0]}"
    except requests.exceptions.ConnectionError as e_conn:
        result["status"] = "âŒ è¿æ¥é”™è¯¯"
        result["error"] = f"æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: {str(e_conn).splitlines()[0]}"
    except requests.RequestException as e_req: 
        result["status"] = "âŒ è¯·æ±‚é”™è¯¯"
        result["error"] = f"è¯·æ±‚æœŸé—´å‘ç”Ÿé”™è¯¯: {str(e_req).splitlines()[0]}"
    except Exception as e_gen: 
        result["status"] = "âŒ å†…éƒ¨å¤„ç†é”™è¯¯"
        result["error"] = f"æ£€æŸ¥çŠ¶æ€æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e_gen).splitlines()[0]}"
        print(f"--- åœ¨ check_website_status å‡½æ•°ä¸­é’ˆå¯¹URL {url} å‘ç”Ÿæ„å¤–é”™è¯¯ ---")
        traceback.print_exc() 
        print(f"--- æ„å¤–é”™è¯¯ç»“æŸ {url} ---")
    
    print(f"{result['status']} - {url} | é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    return result

def get_status_priority(status_str: str) -> int:
    """æ ¹æ®çŠ¶æ€å­—ç¬¦ä¸²è¿”å›æ’åºä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°è¶Šé å‰"""
    if not status_str: status_str = "â“" # å¤„ç† None æˆ–ç©ºçŠ¶æ€
    if status_str.startswith("âŒ"): return 0  # æ‰€æœ‰é”™è¯¯ (è¶…æ—¶, SSL, è¿æ¥, è¯·æ±‚, å†…éƒ¨, çº¿ç¨‹)
    if status_str.startswith("ğŸš«"): return 1  # æœªæ‰¾åˆ° (404)
    if status_str.startswith("âš ï¸"): return 2  # è­¦å‘Š/æœ‰ç‰¹å®šçŠ¶æ€ç çš„å¼‚å¸¸
    if status_str.startswith("â†ªï¸"): return 3  # é‡å®šå‘ (ä¹Ÿè§†ä¸ºéœ€è¦å…³æ³¨)
    if status_str.startswith("â“"): return 4  # æœªçŸ¥çŠ¶æ€
    if status_str.startswith("âœ…"): return 5  # æ­£å¸¸ (æ’åœ¨æœ€å)
    return 6 # å…¶ä»–ä»»ä½•æœªé¢„è§çš„çŠ¶æ€

def _generate_markdown_table_for_column(result_list: list, column_title: str) -> str:
    """ä¸ºå•æ ç”ŸæˆMarkdownè¡¨æ ¼å­—ç¬¦ä¸²"""
    if not result_list:
        return ""

    # ä½¿ç”¨ä¸‰çº§æˆ–å››çº§æ ‡é¢˜ä½œä¸ºåˆ†æ æ ‡é¢˜
    table_content = f"### {column_title}\n\n" 
    
    # è¡¨å¤´ï¼Œæ‰€æœ‰å†…å®¹ç”¨ <small> åŒ…è£¹
    table_header = "| <small>URL</small> | <small>çŠ¶æ€</small> | <small>çŠ¶æ€ç </small> | <small>å“åº”æ—¶é—´</small> | <small>æœ€åæ£€æŸ¥ (UTC)</small> |\n"
    table_alignment = "|:-----|:-------|:----------|:---------------|:--------------------|\n" # æŒ‡å®šå¯¹é½æ–¹å¼
    table_content += table_header + table_alignment

    rows_md = []
    for result in result_list:
        # è¡¨æ ¼æ•°æ®ä¹Ÿç”¨ <small> åŒ…è£¹
        status_code_display = f"<small>{result.get('status_code', 'N/A')}</small>"
        response_time_display = f"<small>{result['response_time']}</small>"
        timestamp_display = f"<small>{result['timestamp']}</small>"
        status_display = f"<small>{result.get('status', 'â“')}</small>"

        url_display_raw = result['url']
        # åˆ›å»ºå¯ç‚¹å‡»çš„URLé“¾æ¥
        url_markdown = f"[{url_display_raw}]({url_display_raw})"
        if 'final_url' in result and result['final_url'] != result['url']:
            # å¦‚æœå‘ç”Ÿé‡å®šå‘ï¼Œä½¿ç”¨ <br> æ¢è¡Œå¹¶ç”¨ <sub> æ˜¾ç¤ºæœ€ç»ˆURLï¼Œä½¿å…¶æ›´å°
            url_markdown += f"<br><sub>â†³ æœ€ç»ˆ: [{result['final_url']}]({result['final_url']})</sub>"
        
        url_display_final = f"<small>{url_markdown}</small>"

        row = f"| {url_display_final} | {status_display} | {status_code_display} | {response_time_display} | {timestamp_display} |"
        rows_md.append(row)
    
    table_content += "\n".join(rows_md) + "\n" # æ¯ä¸ªè¡¨æ ¼ååŠ ä¸€ä¸ªæ¢è¡Œ
    return table_content

def update_readme(results: list, readme_file: str = README_FILENAME):
    """å°†çŠ¶æ€ç»“æœæ›´æ–°åˆ° README.mdï¼Œå®ç°æ’åºã€ç¼©å°å­—ä½“å’Œä¸¤æ å¸ƒå±€"""
    if not results:
        print("âš ï¸ æ²¡æœ‰ç»“æœå¯ä»¥æ›´æ–°åˆ°READMEã€‚")
        content = f"# ç½‘ç«™çŠ¶æ€ç›‘æ§\n\næœ€åæ£€æŸ¥æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
        content += "å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•ç½‘ç«™ï¼Œæˆ–æ‰€æœ‰ç›‘æ§ç»“æœå‡ä¸ºç©ºã€‚\n"
        content += f"\n\nç”± {USER_AGENT} ç›‘æ§\n"
        try:
            with open(readme_file, "w", encoding="utf-8") as f:
                f.write(content)
            print(f"âœ… å·²ä½¿ç”¨ç©ºçŠ¶æ€æ›´æ–° {readme_file}ã€‚")
        except IOError as e:
            print(f"âŒ å†™å…¥ç©ºçš„ {readme_file} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

    # 1. æ’åºç»“æœï¼šä¸æ­£å¸¸çš„åœ¨å‰ï¼Œä¼˜å…ˆçº§ç›¸åŒæ—¶æŒ‰URLå­—æ¯é¡ºåºæ’åº
    results.sort(key=lambda r: (get_status_priority(r.get('status', 'â“')), r['url']))

    # å‡†å¤‡READMEçš„æ•´ä½“å†…å®¹
    readme_content = f"# ç½‘ç«™çŠ¶æ€ç›‘æ§\n\næœ€åæ£€æŸ¥æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
    
    # ä½¿ç”¨HTML divå’ŒFlexboxå®ç°ä¸¤æ å¸ƒå±€
    # flex-wrap: wrap; å…è®¸åœ¨å°å±å¹•ä¸Šè‡ªåŠ¨æ¢è¡Œï¼ˆå †å ï¼‰
    # gap: 20px; æ˜¯æ é—´è·
    readme_content += '<div style="display: flex; flex-direction: row; flex-wrap: wrap; gap: 20px;">\n'

    num_results = len(results)
    # å‘ä¸Šå–æ•´ï¼Œç¡®ä¿ç¬¬ä¸€æ åœ¨å¥‡æ•°æ—¶è·å¾—å¤šä¸€ä¸ª
    mid_point = math.ceil(num_results / 2.0)
    
    results_col1 = results[:int(mid_point)]
    results_col2 = results[int(mid_point):]

    # ç”Ÿæˆç¬¬ä¸€æ çš„Markdownè¡¨æ ¼
    # flex: 1; è®©æ è‡ªåŠ¨åˆ†é…ç©ºé—´
    # min-width: 350px; è®¾ç½®ä¸€ä¸ªæœ€å°å®½åº¦ï¼Œæœ‰åŠ©äºå“åº”å¼è°ƒæ•´
    # æ³¨æ„ï¼šMarkdownè¡¨æ ¼éœ€è¦åœ¨HTMLå—çº§æ ‡ç­¾åæœ‰ç©ºè¡Œæ‰èƒ½è¢«æ­£ç¡®æ¸²æŸ“
    readme_content += '<div style="flex: 1; min-width: 350px;">\n\n' 
    readme_content += _generate_markdown_table_for_column(results_col1, "ç›‘æ§åˆ—è¡¨ (1)")
    readme_content += '\n</div>\n'

    # å¦‚æœç¬¬äºŒæ æœ‰å†…å®¹ï¼Œåˆ™ç”Ÿæˆç¬¬äºŒæ 
    if results_col2:
        readme_content += '<div style="flex: 1; min-width: 350px;">\n\n'
        readme_content += _generate_markdown_table_for_column(results_col2, "ç›‘æ§åˆ—è¡¨ (2)")
        readme_content += '\n</div>\n'
    
    readme_content += '</div>\n' # å…³é—­Flexboxå®¹å™¨
    readme_content += f"\n\nç”± {USER_AGENT} ç›‘æ§\n"

    try:
        with open(readme_file, "w", encoding="utf-8") as f:
            f.write(readme_content)
        print(f"âœ… å·²å°†æœ€æ–°çš„ç½‘ç«™çŠ¶æ€ ({num_results} ä¸ªç«™ç‚¹ï¼Œå·²æ’åºå’Œåˆ†æ ) æ›´æ–°åˆ° {readme_file}ã€‚")
    except IOError as e:
        print(f"âŒ å†™å…¥ {readme_file} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# --- ä¸»ç¨‹åºé€»è¾‘ ---
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(f"ç”¨æ³•: python {os.path.basename(__file__)} <url_filename>")
        sys.exit(1)

    url_source_filename = sys.argv[1]

    print(f"--- ç¬¬1æ­¥: å¤„ç†URLæ–‡ä»¶: {url_source_filename} ---")
    urls_to_check = process_url_file(url_source_filename)

    if not urls_to_check:
        print(f"âš ï¸ åœ¨ {url_source_filename} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URLå¯ä¾›æ£€æŸ¥ã€‚æ­£åœ¨é€€å‡ºã€‚")
        sys.exit(1)
    print(f"æ‰¾åˆ° {len(urls_to_check)} ä¸ªå”¯ä¸€ä¸”æœ‰æ•ˆçš„URLè¿›è¡Œç›‘æ§ã€‚")

    print(f"\n--- ç¬¬2æ­¥: æ£€æŸ¥ç½‘ç«™çŠ¶æ€ (æœ€å¤§å¹¶å‘æ•°={MAX_WORKERS}) ---")
    results_map = {}
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_website_status, url): url for url in urls_to_check}
        for future in as_completed(future_to_url):
            original_url = future_to_url[future]
            try:
                result = future.result() 
                results_map[original_url] = result
            except Exception as exc: 
                print(f"âŒ URL {original_url} åœ¨çº¿ç¨‹æ‰§è¡ŒæœŸé—´äº§ç”Ÿäº†ä¸€ä¸ªæœªèƒ½æ•è·çš„å¼‚å¸¸:")
                traceback.print_exc() 
                error_result = {
                    "url": original_url,
                    "status": "âŒ çº¿ç¨‹æ‰§è¡Œé”™è¯¯", 
                    "status_code": None,
                    "response_time": "N/A",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "error": f"çº¿ç¨‹å†…æœªæ•è·çš„å¼‚å¸¸: {str(exc)}" 
                }
                results_map[original_url] = error_result
    
    ordered_results = [results_map[url] for url in urls_to_check if url in results_map] # ä¿æŒåŸå§‹é¡ºåºï¼ˆå¦‚æœéœ€è¦ï¼‰æˆ–åç»­æ’åº

    print(f"\n--- ç¬¬3æ­¥: æ›´æ–° {README_FILENAME} ---")
    update_readme(ordered_results, readme_file=README_FILENAME) # update_readmeå†…éƒ¨ä¼šè¿›è¡Œæ’åº

    print(f"\n--- ç¬¬4æ­¥: æ›´æ–°URLæ–‡ä»¶ {url_source_filename} (ç§»é™¤404çŠ¶æ€çš„URL) ---")
    valid_urls_after_check = []
    removed_404_count = 0
    # æ³¨æ„ï¼šè¿™é‡Œçš„ ordered_results æ˜¯æœªç»è¿‡ update_readme å†…éƒ¨æ’åºçš„ç‰ˆæœ¬
    # å¦‚æœå¸Œæœ›ç§»é™¤404åŸºäºæ’åºåçš„åˆ—è¡¨ï¼Œåˆ™åº”åœ¨ update_readme åæˆ–ä½¿ç”¨å…¶æ’åºç»“æœ
    # ä½†é€šå¸¸ç§»é™¤404æ˜¯åŸºäºåŸå§‹æ£€æŸ¥ç»“æœï¼Œä¸æ˜¾ç¤ºé¡ºåºæ— å…³
    for result_key in urls_to_check: # è¿­ä»£åŸå§‹æ£€æŸ¥çš„URLåˆ—è¡¨ï¼Œä»¥ä¿æŒä¸€è‡´æ€§
        result = results_map.get(result_key)
        if result: # ç¡®ä¿ç»“æœå­˜åœ¨
            if result.get('status_code') != 404:
                valid_urls_after_check.append(result['url'])
            else:
                print(f"ğŸ—‘ï¸ æ ‡è®° {result['url']} å› ä¸º404çŠ¶æ€å°†ä» {url_source_filename} ä¸­ç§»é™¤ã€‚")
                removed_404_count +=1
        else:
            # å¦‚æœæŸä¸ªURLæ²¡æœ‰ç»“æœï¼ˆä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œé™¤éçº¿ç¨‹æ± é€»è¾‘æœ‰è¯¯æˆ–URLè¢«è¿‡æ»¤æ‰ï¼‰
            # å¯ä»¥é€‰æ‹©ä¿ç•™å®ƒï¼Œæˆ–è€…ä¹Ÿå°†å…¶è§†ä¸ºé—®é¢˜URL
            print(f"âš ï¸ URL {result_key} æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æ£€æŸ¥ç»“æœï¼Œå°†ä»æ–‡ä»¶ä¸­ä¿ç•™ã€‚")
            valid_urls_after_check.append(result_key)


    if removed_404_count > 0:
        # ç¡®ä¿ valid_urls_after_check ä¸­çš„URLé¡ºåºä¸ urls_to_check ä¸€è‡´ï¼ˆç§»é™¤äº†404çš„ï¼‰
        # ä¸Šé¢çš„å¾ªç¯å·²ç»ä¿è¯äº†è¿™ä¸€ç‚¹
        print(f"ğŸ”„ æ­£åœ¨æ›´æ–° {url_source_filename}: ç§»é™¤ {removed_404_count} ä¸ª404çŠ¶æ€çš„URLã€‚")
        try:
            with open(url_source_filename, 'w', encoding='utf-8') as file:
                file.write('\n'.join(valid_urls_after_check) + '\n')
            print(f"âœ… {url_source_filename} å·²æ›´æ–°ã€‚ç°åœ¨åŒ…å« {len(valid_urls_after_check)} ä¸ªURLã€‚")
        except IOError as e:
            print(f"âŒ å°†æ›´æ–°åçš„URLåˆ—è¡¨å†™å…¥ {url_source_filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        print(f"âœ… æœªå‘ç°éœ€è¦ä» {url_source_filename} ä¸­ç§»é™¤çš„404çŠ¶æ€URLã€‚URLåˆ—è¡¨æœªä½œæ›´æ”¹ã€‚")
    
    print("\nç›‘æ§è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
