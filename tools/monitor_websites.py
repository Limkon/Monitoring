import os
import sys
import requests
import time
from datetime import datetime
from urllib.parse import urlparse

def normalize_url(url):
    """æ£€æŸ¥ URL æ˜¯å¦åŒ…å«åè®®ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™æ·»åŠ  https://"""
    parsed_url = urlparse(url)
    if not parsed_url.scheme:
        return "https://" + url
    return url

def looks_like_url(url):
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦çœ‹èµ·æ¥åƒ URL"""
    try:
        parsed_url = urlparse(url)
        return bool(parsed_url.netloc) and bool(parsed_url.scheme or parsed_url.path)
    except Exception:
        return False

def check_website_status(url):
    """æ£€æŸ¥ç½‘ç«™çŠ¶æ€å¹¶è¿”å›ç»“æœ"""
    try:
        start_time = time.time()
        response = requests.get(url, timeout=10)
        end_time = time.time()
        response_time = (end_time - start_time) * 1000

        status_code = response.status_code
        status = "âœ… Up" if status_code == 200 else f"âš ï¸ Down (Status: {status_code})"

        result = {
            "url": url,
            "status": status,
            "status_code": status_code,
            "response_time": f"{response_time:.2f} ms",
            "timestamp": datetime.utcnow().isoformat()
        }
        print(f"{result['status']} - {url} | Code: {status_code} | Time: {result['response_time']}")
        return result

    except requests.RequestException as e:
        result = {
            "url": url,
            "status": "âŒ Down (Error)",
            "status_code": None,
            "response_time": "N/A",
            "timestamp": datetime.utcnow().isoformat(),
            "error": str(e)
        }
        print(f"âŒ Down - {url} | Error: {str(e)}")
        return result

def update_readme(results, readme_file="README.md"):
    """å°†çŠ¶æ€ç»“æœæ›´æ–°åˆ° README.md"""
    header = "# Website Status\n\nBelow is the latest status of monitored websites:\n\n"
    table_header = "| URL | Status | Status Code | Response Time | Last Checked |\n|-----|--------|-------------|---------------|--------------|\n"
    
    rows = []
    for result in results:
        status_code = result.get('status_code', 'N/A')
        row = f"| {result['url']} | {result['status']} | {status_code} | {result['response_time']} | {result['timestamp']} |"
        rows.append(row)

    content = header + table_header + "\n".join(rows) + "\n"
    
    with open(readme_file, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"âœ… Updated {readme_file} with latest website status.")

def remove_duplicates_and_update_file(filename):
    """å»é™¤é‡å¤ã€ç©ºè¡Œï¼Œåˆ é™¤ä¸ç¬¦åˆ URL æ ¼å¼çš„è¡Œï¼Œå¹¶è¡¥å…¨ URL åè®®"""
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            urls = [line.strip() for line in file.readlines()]

        # å»é™¤ç©ºè¡Œå’Œä¸ç¬¦åˆ URL æ ¼å¼çš„è¡Œ
        urls = [url for url in urls if url and looks_like_url(url)]

        if not urls:
            print(f"âš ï¸ No valid URLs found in {filename}. Exiting.")
            return []

        # å…ˆè¡¥å…¨åè®®ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        fixed_urls = [normalize_url(url) for url in urls]

        # å»é‡ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        unique_urls = []
        seen = set()
        for url in fixed_urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        # ä»…å½“æœ‰ä¿®æ”¹æ—¶æ‰æ›´æ–°æ–‡ä»¶
        if unique_urls != urls:
            print(f"ğŸ”„ Updating {filename}: Fixing URLs, removing {len(urls) - len(unique_urls)} duplicates and blank lines...")
            with open(filename, 'w', encoding='utf-8') as file:
                file.write('\n'.join(unique_urls) + '\n')
            print(f"âœ… {filename} has been updated successfully.")
        else:
            print(f"âœ… No changes needed for {filename}. Already clean.")

        return unique_urls

    except Exception as e:
        print(f"âŒ Error processing {filename}: {e}")
        return []

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python monitor_websites.py <filename>")
        sys.exit(1)

    filename = sys.argv[1]

    # 1ï¸âƒ£ å»é‡ã€å»ç©ºè¡Œå¹¶è§„èŒƒ URL
    unique_urls = remove_duplicates_and_update_file(filename)

    if not unique_urls:
        sys.exit(1)

    # 2ï¸âƒ£ æ£€æŸ¥æ¯ä¸ªç½‘ç«™çš„çŠ¶æ€
    results = []
    for url in unique_urls:
        result = check_website_status(url)
        results.append(result)

    # 3ï¸âƒ£ æ›´æ–° README.md
    update_readme(results)

    # 4ï¸âƒ£ é‡æ–°ä¿å­˜å¤„ç†åçš„ URLs
    with open(filename, 'w', encoding='utf-8') as file:
        file.write('\n'.join(unique_urls) + '\n')
    print(f"âœ… Updated {filename} with cleaned URLs.")
