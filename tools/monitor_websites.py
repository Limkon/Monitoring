import os
import sys
import requests
import time
import logging
import traceback
from datetime import datetime, timezone
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Optional, Union

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class MonitorConfig:
    """ç›‘æ§é…ç½®ç±»"""
    REQUEST_TIMEOUT = 10
    README_FILENAME = "README.md"
    USER_AGENT = "WebsiteStatusMonitor/1.0 (+https://github.com/your_username/your_repo)"
    MAX_WORKERS = 10

class WebsiteMonitor:
    """ç½‘ç«™çŠ¶æ€ç›‘æ§æ ¸å¿ƒç±»"""

    def __init__(self, url_filename: str):
        self.url_filename = Path(url_filename)
        self.readme_filename = Path(MonitorConfig.README_FILENAME)

    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ– URL"""
        url = url.strip()
        try:
            parsed_url = urlparse(url)
            if not parsed_url.scheme:
                return "https://" + url
            return url
        except Exception:
            return url

    def looks_like_url(self, url_str: str) -> bool:
        """ç®€å•çš„ URL æ ¼å¼éªŒè¯"""
        if not isinstance(url_str, str) or not url_str.strip():
            return False
        try:
            parsed_url = urlparse(url_str.strip())
            return bool(parsed_url.netloc and '.' in parsed_url.netloc) and \
                   bool(parsed_url.scheme or parsed_url.path or parsed_url.netloc)
        except ValueError:
            return False

    def process_url_file(self) -> List[str]:
        """è¯»å–å¹¶æ¸…æ´— URL æ–‡ä»¶ï¼Œå»é™¤é‡å¤é¡¹"""
        if not self.url_filename.exists():
            logger.error(f"âŒ é”™è¯¯: æ–‡ä»¶ {self.url_filename} æœªæ‰¾åˆ°ã€‚")
            return []

        try:
            content = self.url_filename.read_text(encoding='utf-8')
            initial_urls = [line.strip() for line in content.splitlines()]
        except Exception as e:
            logger.error(f"âŒ è¯»å– {self.url_filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

        valid_format_urls = [url for url in initial_urls if self.looks_like_url(url)]
        normalized_urls = [self.normalize_url(url) for url in valid_format_urls]
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        unique_urls = []
        seen = set()
        for url in normalized_urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ–‡ä»¶
        current_file_ideal_lines = []
        try:
            current_content = self.url_filename.read_text(encoding='utf-8')
            current_file_ideal_lines = [line.strip() for line in current_content.splitlines() if line.strip()]
        except Exception:
            pass

        if unique_urls != current_file_ideal_lines:
            logger.info(f"ğŸ”„ æ­£åœ¨æ›´æ–° {self.url_filename}...")
            try:
                self.url_filename.write_text('\n'.join(unique_urls) + '\n', encoding='utf-8')
                logger.info(f"âœ… {self.url_filename} å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(unique_urls)} ä¸ªURLã€‚")
            except IOError as e:
                logger.error(f"âŒ æ›´æ–° {self.url_filename} æ—¶å†™å…¥é”™è¯¯: {e}")
                return initial_urls # å†™å…¥å¤±è´¥è¿”å›åŸå§‹åˆ—è¡¨ï¼Œå°½é‡ç»§ç»­
        else:
            logger.info(f"âœ… {self.url_filename} æ— éœ€ç»“æ„æ€§æ›´æ”¹ï¼Œå·²æ˜¯æœ€æ–°ã€‚")
        
        return unique_urls

    def check_website_status(self, url: str) -> Dict:
        """æ£€æŸ¥å•ä¸ªç½‘ç«™çš„çŠ¶æ€"""
        headers = {'User-Agent': MonitorConfig.USER_AGENT}
        result = {
            "url": url, 
            "status_code": None, 
            "response_time": "N/A",
            "timestamp": datetime.now(timezone.utc).isoformat(), 
            "error": None, 
            "status": "â“ æœªçŸ¥çŠ¶æ€"
        }
        
        try:
            start_time = time.time()
            response = requests.get(url, timeout=MonitorConfig.REQUEST_TIMEOUT, headers=headers, allow_redirects=True)
            end_time = time.time()
            
            response_time_ms = (end_time - start_time) * 1000
            result["status_code"] = response.status_code
            result["response_time"] = f"{response_time_ms:.2f} ms"
            
            if response.url != url:
                result["final_url"] = response.url

            if 200 <= response.status_code < 300:
                result["status"] = "âœ… æ­£å¸¸"
            elif 300 <= response.status_code < 400:
                result["status"] = f"â†ªï¸ é‡å®šå‘ ({response.status_code})"
            elif response.status_code == 404:
                result["status"] = "ğŸš« æœªæ‰¾åˆ° (404)"
            else:
                result["status"] = f"âš ï¸ å¼‚å¸¸ (çŠ¶æ€: {response.status_code})"
            
            final_url_info = f" | æœ€ç»ˆURL: {result['final_url']}" if "final_url" in result else ""
            logger.info(f"{result['status']} - {url} | Code: {result['status_code']} | Time: {result['response_time']}{final_url_info}")

        except requests.Timeout:
            result["status"] = "âŒ å¤ªæ…¢ (è¶…æ—¶)"
            result["error"] = "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.SSLError as e:
            result["status"] = "âŒ SSLé”™è¯¯"
            result["error"] = f"SSLé—®é¢˜: {str(e).splitlines()[0]}"
        except requests.exceptions.ConnectionError as e:
            result["status"] = "âŒ è¿æ¥é”™è¯¯"
            result["error"] = f"è¿æ¥é—®é¢˜: {str(e).splitlines()[0]}"
        except requests.RequestException as e:
            result["status"] = "âŒ è¯·æ±‚é”™è¯¯"
            result["error"] = f"è¯·æ±‚é—®é¢˜: {str(e).splitlines()[0]}"
        except Exception as e:
            result["status"] = "âŒ å†…éƒ¨å¤„ç†é”™è¯¯"
            result["error"] = f"æ„å¤–é”™è¯¯: {str(e).splitlines()[0]}"
            logger.error(f"--- æ£€æŸ¥ {url} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ ---")
            logger.error(traceback.format_exc())

        if result["error"]:
            logger.warning(f"{result['status']} - {url} | é”™è¯¯: {result.get('error')}")
            
        return result

    def get_status_priority(self, status_str: str) -> int:
        """è·å–çŠ¶æ€ä¼˜å…ˆçº§ç”¨äºæ’åº"""
        if not status_str: status_str = "â“"
        if status_str.startswith("âŒ"): return 0
        if status_str.startswith("ğŸš«"): return 1
        if status_str.startswith("âš ï¸"): return 2
        if status_str.startswith("â†ªï¸"): return 3
        if status_str.startswith("â“"): return 4
        if status_str.startswith("âœ…"): return 5
        return 6

    def update_readme(self, results: List[Dict]):
        """æ›´æ–° README.md æ–‡ä»¶"""
        if not results:
            logger.warning("âš ï¸ æ²¡æœ‰ç»“æœå¯ä»¥æ›´æ–°åˆ°READMEã€‚")
            content = f"# ç½‘ç«™çŠ¶æ€ç›‘æ§\n\næœ€åæ£€æŸ¥æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
            content += "å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•ç½‘ç«™ï¼Œæˆ–æ‰€æœ‰ç›‘æ§ç»“æœå‡ä¸ºç©ºã€‚\n"
            content += f"\n\nç”± {MonitorConfig.USER_AGENT} ç›‘æ§\n"
            try:
                self.readme_filename.write_text(content, encoding="utf-8")
                logger.info(f"âœ… å·²ä½¿ç”¨ç©ºçŠ¶æ€æ›´æ–° {self.readme_filename}ã€‚")
            except IOError as e:
                logger.error(f"âŒ å†™å…¥ç©ºçš„ {self.readme_filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return

        # æ’åºç»“æœ
        results.sort(key=lambda r: (self.get_status_priority(r.get('status', 'â“')), r['url']))

        readme_content = f"# ç½‘ç«™çŠ¶æ€ç›‘æ§\n\næœ€åæ£€æŸ¥æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
        
        table_header = "| <small>URL</small> | <small>çŠ¶æ€</small> | <small>çŠ¶æ€ç </small> | <small>å“åº”æ—¶é—´</small> | <small>æœ€åæ£€æŸ¥ (UTC)</small> |\n"
        table_alignment = "|:-----|:-------|:----------|:---------------|:--------------------|\n"
        readme_content += table_header + table_alignment

        rows_md = []
        for result in results:
            status_code_display = f"<small>{result.get('status_code', 'N/A')}</small>"
            response_time_display = f"<small>{result['response_time']}</small>"
            timestamp_display = f"<small>{result['timestamp']}</small>"
            status_display = f"<small>{result.get('status', 'â“')}</small>"

            url_display_raw = result['url']
            url_markdown = f"[{url_display_raw}]({url_display_raw})"
            if 'final_url' in result and result['final_url'] != result['url']:
                url_markdown += f"<br><sub>â†³ æœ€ç»ˆ: [{result['final_url']}]({result['final_url']})</sub>"
            
            url_display_final = f"<small>{url_markdown}</small>"

            row = f"| {url_display_final} | {status_display} | {status_code_display} | {response_time_display} | {timestamp_display} |"
            rows_md.append(row)
        
        readme_content += "\n".join(rows_md) + "\n"
        readme_content += f"\n\nç”± {MonitorConfig.USER_AGENT} ç›‘æ§\n"

        try:
            self.readme_filename.write_text(readme_content, encoding="utf-8")
            logger.info(f"âœ… å·²å°†æœ€æ–°çš„ç½‘ç«™çŠ¶æ€ ({len(results)} ä¸ªç«™ç‚¹ï¼Œå·²æ’åº) æ›´æ–°åˆ° {self.readme_filename}ã€‚")
        except IOError as e:
            logger.error(f"âŒ å†™å…¥ {self.readme_filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def run(self):
        """æ‰§è¡Œç›‘æ§æµç¨‹"""
        logger.info(f"--- ç¬¬1æ­¥: å¤„ç†URLæ–‡ä»¶: {self.url_filename} ---")
        urls_to_check = self.process_url_file()
        if not urls_to_check:
            logger.warning(f"âš ï¸ åœ¨ {self.url_filename} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URLå¯ä¾›æ£€æŸ¥ã€‚æ­£åœ¨é€€å‡ºã€‚")
            return

        logger.info(f"æ‰¾åˆ° {len(urls_to_check)} ä¸ªå”¯ä¸€ä¸”æœ‰æ•ˆçš„URLè¿›è¡Œç›‘æ§ã€‚")
        logger.info(f"\n--- ç¬¬2æ­¥: æ£€æŸ¥ç½‘ç«™çŠ¶æ€ (æœ€å¤§å¹¶å‘æ•°={MonitorConfig.MAX_WORKERS}) ---")
        
        results_map = {}
        with ThreadPoolExecutor(max_workers=MonitorConfig.MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self.check_website_status, url): url for url in urls_to_check}
            for future in as_completed(future_to_url):
                original_url = future_to_url[future]
                try:
                    result = future.result()
                    results_map[original_url] = result
                except Exception as exc:
                    logger.error(f"âŒ URL {original_url} åœ¨çº¿ç¨‹æ‰§è¡ŒæœŸé—´äº§ç”Ÿäº†ä¸€ä¸ªæœªèƒ½æ•è·çš„å¼‚å¸¸: {exc}")
                    logger.error(traceback.format_exc())
                    error_result = {
                        "url": original_url, "status": "âŒ çº¿ç¨‹æ‰§è¡Œé”™è¯¯", 
                        "status_code": None, "response_time": "N/A",
                        "timestamp": datetime.now(timezone.utc).isoformat(), 
                        "error": f"çº¿ç¨‹å†…æœªæ•è·: {str(exc)}"
                    }
                    results_map[original_url] = error_result

        ordered_results = [results_map[url] for url in urls_to_check if url in results_map]

        logger.info(f"\n--- ç¬¬3æ­¥: æ›´æ–° {self.readme_filename} ---")
        self.update_readme(ordered_results)

        logger.info(f"\n--- ç¬¬4æ­¥: æ›´æ–°URLæ–‡ä»¶ {self.url_filename} (ç§»é™¤404) ---")
        valid_urls_after_check = []
        removed_404_count = 0
        for result_key in urls_to_check:
            result = results_map.get(result_key)
            if result:
                if result.get('status_code') != 404:
                    valid_urls_after_check.append(result['url'])
                else:
                    logger.info(f"ğŸ—‘ï¸ æ ‡è®° {result['url']} å› 404å°†ç§»é™¤ã€‚")
                    removed_404_count += 1
            else:
                logger.warning(f"âš ï¸ URL {result_key} æ— æ£€æŸ¥ç»“æœï¼Œå°†ä¿ç•™ã€‚")
                valid_urls_after_check.append(result_key)

        if removed_404_count > 0:
            logger.info(f"ğŸ”„ æ›´æ–° {self.url_filename}: ç§»é™¤ {removed_404_count} ä¸ª404 URLã€‚")
            try:
                self.url_filename.write_text('\n'.join(valid_urls_after_check) + '\n', encoding='utf-8')
                logger.info(f"âœ… {self.url_filename} å·²æ›´æ–°ï¼Œå« {len(valid_urls_after_check)} URLã€‚")
            except IOError as e:
                logger.error(f"âŒ å†™å…¥ {self.url_filename} å¤±è´¥: {e}")
        else:
            logger.info(f"âœ… æ— 404 URLéœ€ç§»é™¤äº {self.url_filename}ã€‚")
        
        logger.info("\nç›‘æ§è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(f"ç”¨æ³•: python {os.path.basename(__file__)} <url_filename>")
        sys.exit(1)
    
    url_source_filename = sys.argv[1]
    monitor = WebsiteMonitor(url_source_filename)
    monitor.run()
