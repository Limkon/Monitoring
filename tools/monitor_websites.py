import os
import sys
import requests
import time
from datetime import datetime, timezone # å¼•å…¥ timezone
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed # ç”¨äºå¹¶å‘

# --- é…ç½®å¸¸é‡ ---
REQUEST_TIMEOUT = 10  # seconds
README_FILENAME = "README.md"
USER_AGENT = "WebsiteStatusMonitor/1.0 (+https://github.com/your_username/your_repo)" # æ›¿æ¢ä¸ºæ‚¨çš„ä¿¡æ¯
MAX_WORKERS = 10 # å¹¶å‘æ£€æŸ¥çš„æœ€å¤§çº¿ç¨‹æ•° (å¯ä»¥æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´, e.g., os.cpu_count() * 2)

def normalize_url(url):
    """æ£€æŸ¥ URL æ˜¯å¦åŒ…å«åè®®ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™æ·»åŠ  https://"""
    parsed_url = urlparse(url.strip()) # strip url before parsing
    if not parsed_url.scheme:
        return "https://" + url.strip()
    return url.strip()

def looks_like_url(url_str):
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦çœ‹èµ·æ¥åƒ URL"""
    if not isinstance(url_str, str) or not url_str.strip():
        return False
    try:
        parsed_url = urlparse(url_str.strip())
        # ç¡®ä¿æœ‰ç½‘ç»œä½ç½® (netloc) å¹¶ä¸”æœ‰åè®® (scheme) æˆ–è·¯å¾„ (path)
        # è¿›ä¸€æ­¥æ£€æŸ¥ netloc æ˜¯å¦åŒ…å«ç‚¹ï¼Œä»¥æ’é™¤åƒ "localhost" è¿™æ ·çš„ç®€å•è¯ï¼ˆé™¤éç¡®å®éœ€è¦ç›‘æ§localhostï¼‰
        return bool(parsed_url.netloc and '.' in parsed_url.netloc) and \
               bool(parsed_url.scheme or parsed_url.path or parsed_url.netloc)
    except ValueError: # urlparse can raise ValueError for very malformed URLs
        return False

def check_website_status(url):
    """æ£€æŸ¥ç½‘ç«™çŠ¶æ€å¹¶è¿”å›ç»“æœå­—å…¸"""
    headers = {'User-Agent': USER_AGENT}
    result = {
        "url": url,
        "status_code": None,
        "response_time": "N/A",
        "timestamp": datetime.now(timezone.utc).isoformat(), # ä½¿ç”¨ timezone-aware UTC time
        "error": None
    }
    try:
        start_time = time.time()
        # å…è®¸é‡å®šå‘ï¼Œä½†è®°å½•æœ€ç»ˆçš„URLï¼ˆrequestsé»˜è®¤å¤„ç†é‡å®šå‘ï¼‰
        response = requests.get(url, timeout=REQUEST_TIMEOUT, headers=headers, allow_redirects=True)
        end_time = time.time()
        response_time_ms = (end_time - start_time) * 1000

        result["status_code"] = response.status_code
        result["response_time"] = f"{response_time_ms:.2f} ms"
        # å¦‚æœå‘ç”Ÿé‡å®šå‘ï¼Œresponse.url ä¼šæ˜¯æœ€ç»ˆçš„URL
        if response.url != url:
            result["final_url"] = response.url # è®°å½•æœ€ç»ˆURL

        if 200 <= response.status_code < 300:
            result["status"] = "âœ… Up"
        elif 300 <= response.status_code < 400:
            result["status"] = f"â†ªï¸ Redirect ({response.status_code})"
        elif response.status_code == 404:
            result["status"] = "ğŸš« Not Found (404)"
        else:
            result["status"] = f"âš ï¸ Down (Status: {response.status_code})"

        # æ‰“å°æ—¶ä½¿ç”¨åŸå§‹è¯·æ±‚çš„ URL
        print(f"{result['status']} - {url} | Code: {result['status_code']} | Time: {result['response_time']}" +
              (f" | Final URL: {result['final_url']}" if "final_url" in result else ""))
        return result

    except requests.Timeout:
        result["status"] = "âŒ Down (Timeout)"
        result["error"] = "Request timed out"
    except requests.RequestException as e:
        result["status"] = "âŒ Down (Error)"
        result["error"] = str(e).splitlines()[0] #å–é”™è¯¯ä¿¡æ¯çš„ç¬¬ä¸€è¡Œï¼Œé¿å…è¿‡é•¿
    
    print(f"{result['status']} - {url} | Error: {result.get('error', 'Unknown')}")
    return result


def update_readme(results, readme_file=README_FILENAME):
    """å°†çŠ¶æ€ç»“æœæ›´æ–°åˆ° README.md"""
    if not results:
        print("âš ï¸ No results to update README with.")
        return

    # æŒ‰åŸå§‹URLæ’åºï¼ˆå¦‚æœç»“æœæ˜¯å­—å…¸çš„è¯ï¼‰ï¼Œæˆ–è€…å¦‚æœä¼ å…¥çš„æ˜¯åˆ—è¡¨å°±ç›´æ¥ç”¨
    # results.sort(key=lambda r: r['url']) # å¯é€‰ï¼šå¦‚æœéœ€è¦æŒ‰URLå­—æ¯é¡ºåºæ’åº

    header = f"# Website Status\n\nLast checked: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
    table_header = "| URL | Status | Status Code | Response Time | Last Checked (UTC) |\n|-----|--------|-------------|---------------|--------------------|\n"
    
    rows = []
    for result in results:
        status_code_display = result.get('status_code', 'N/A')
        # å¤„ç† final_urlï¼Œå¦‚æœå­˜åœ¨åˆ™æ˜¾ç¤ºï¼Œå¹¶æŒ‡å‘åŸå§‹è¯·æ±‚çš„ URL
        url_display = result['url']
        if 'final_url' in result and result['final_url'] != result['url']:
            url_display = f"[{result['url']}]({result['url']}) ( ìµœì¢…: [{result['final_url']}]({result['final_url']}) )" # Markdown link
        else:
             url_display = f"[{result['url']}]({result['url']})"


        row = f"| {url_display} | {result['status']} | {status_code_display} | {result['response_time']} | {result['timestamp']} |"
        rows.append(row)

    content = header + table_header + "\n".join(rows) + "\n\n" \
              f"Monitored with {USER_AGENT}\n"
    
    try:
        with open(readme_file, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"âœ… Updated {readme_file} with latest website status ({len(results)} sites).")
    except IOError as e:
        print(f"âŒ Error writing to {readme_file}: {e}")

def process_url_file(filename):
    """è¯»å–ã€å»é‡ã€è§„èŒƒåŒ–URLï¼Œå¹¶å†™å›æ–‡ä»¶ï¼ˆå¦‚æœå‘ç”Ÿæ›´æ”¹ï¼‰ã€‚è¿”å›å¤„ç†åçš„URLåˆ—è¡¨ã€‚"""
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            # strip() each line during read
            initial_urls = [line.strip() for line in file.readlines()]
    except FileNotFoundError:
        print(f"âŒ Error: File {filename} not found.")
        return []
    except Exception as e:
        print(f"âŒ Error reading {filename}: {e}")
        return []

    # è¿‡æ»¤çœ‹èµ·æ¥ä¸åƒURLçš„è¡Œå’Œç©ºè¡Œ
    valid_format_urls = [url for url in initial_urls if looks_like_url(url)]
    
    # è§„èŒƒåŒ–URL (è¡¥å…¨åè®®)
    normalized_urls = [normalize_url(url) for url in valid_format_urls]

    # å»é‡ (ä¿æŒåŸå§‹é¡ºåº)
    unique_urls = []
    seen = set()
    for url in normalized_urls:
        if url not in seen:
            seen.add(url)
            unique_urls.append(url)

    # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹æ›´æ”¹ (ä¸ä»…ä»…æ˜¯è¡Œæ•°å˜åŒ–ï¼Œè¿˜è¦è€ƒè™‘é¡ºåºå’Œå†…å®¹)
    # æœ€ç®€å•çš„æ–¹å¼æ˜¯æ¯”è¾ƒå¤„ç†åçš„URLåˆ—è¡¨å’Œåˆå§‹æœ‰æ•ˆURLåˆ—è¡¨ï¼ˆç»è¿‡åŒæ ·å¤„ç†æµç¨‹ï¼‰
    # æˆ–è€…æ¯”è¾ƒæœ€ç»ˆçš„ unique_urls å­—ç¬¦ä¸²å½¢å¼å’Œåˆå§‹æ–‡ä»¶å†…å®¹ï¼ˆå»é™¤æ— æ•ˆè¡Œåï¼‰
    # è¿™é‡Œé‡‡ç”¨ä¸€ç§ç›´æ¥çš„æ–¹å¼ï¼šå¦‚æœ unique_urls å’Œ initial_urlsï¼ˆä»…åŒ…å«æœ‰æ•ˆæ ¼å¼ã€è§„èŒƒåŒ–å’Œå»é‡åï¼‰ä¸åŒï¼Œåˆ™æ›´æ–°ã€‚
    
    # ä¸ºäº†å‡†ç¡®åˆ¤æ–­æ˜¯å¦éœ€è¦å†™å›ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒç”Ÿæˆçš„ unique_urls å’Œæ–‡ä»¶åŸå§‹å†…å®¹å¤„ç†åçš„ç»“æœ
    # å¦‚æœåŸå§‹æ–‡ä»¶å·²ç»å®Œç¾ï¼Œåˆ™ initial_urls å¤„ç†ååº”è¯¥ç­‰äº unique_urls
    # initial_urls_as_processed_for_comparison = []
    # seen_initial = set()
    # for url_orig in [normalize_url(u) for u in initial_urls if looks_like_url(u)]:
    #     if url_orig not in seen_initial:
    #         seen_initial.add(url_orig)
    #         initial_urls_as_processed_for_comparison.append(url_orig)

    # if unique_urls != initial_urls_as_processed_for_comparison:

    # æ›´ç®€å•çš„æ–¹å¼ï¼šæ¯”è¾ƒå†™å…¥å†…å®¹å’ŒåŸå§‹è¯»å–å†…å®¹ï¼ˆå»é™¤ç©ºç™½è¡Œï¼‰
    # å¦ä¸€ç§ç®€å•æ–¹æ³•ï¼šå¦‚æœ unique_urls çš„æ•°é‡æˆ–å†…å®¹ä¸åŸå§‹æœ‰æ•ˆURLåˆ—è¡¨ï¼ˆå¤„ç†å‰ï¼‰ä¸åŒ
    # æˆ–è€…ï¼Œç›´æ¥æ¯”è¾ƒæœ€ç»ˆç”Ÿæˆçš„ unique_urls åˆ—è¡¨å’Œä»æ–‡ä»¶è¯»å…¥å¹¶åˆæ­¥å¤„ç†ï¼ˆstripï¼‰çš„åˆ—è¡¨ã€‚
    # å¦‚æœæ–‡ä»¶å†…å®¹ä¸ `\n`.join(unique_urls) + `\n` ä¸åŒï¼Œåˆ™å†™å…¥ã€‚
    
    original_content_as_string = "\n".join(initial_urls) # ä¿å­˜åŸå§‹å®Œæ•´å†…å®¹ç”¨äºæ¯”è¾ƒ
    new_content_as_string = "\n".join(unique_urls)

    # åªæœ‰å½“å¤„ç†åçš„å†…å®¹ä¸åŸå§‹ï¼ˆç»è¿‡stripçš„ï¼‰å†…å®¹ä¸åŒæ—¶æ‰æ›´æ–°
    # ä¸ºé¿å…å› å°¾éƒ¨æ¢è¡Œç¬¦é—®é¢˜è¯¯åˆ¤ï¼Œæ¯”è¾ƒåˆ—è¡¨æœ¬èº«æˆ–è€…å¤„ç†åçš„å­—ç¬¦ä¸²
    if unique_urls != [normalize_url(u) for u in [s for s in initial_urls if looks_like_url(s)] if u not in seen] or \
       len(unique_urls) != len([s for s in initial_urls if looks_like_url(s) and normalize_url(s) in seen]): # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
        # More robust: compare the string to be written with current file content (after basic cleaning for comparison)
        # For simplicity, if the list of unique URLs is different from what one would get by just reading and stripping, update.
        # This check might be complex. A simpler heuristic: if counts changed or if the actual strings changed.

        # é‡æ–°è¯»å–æ–‡ä»¶ï¼Œå¹¶æ„å»ºä¸€ä¸ªâ€œç†æƒ³çš„â€å½“å‰æ–‡ä»¶å†…å®¹åˆ—è¡¨
        current_file_ideal_lines = []
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f_check:
                current_file_ideal_lines = [line.strip() for line in f_check if line.strip()]
        
        # å¦‚æœ unique_urls ä¸å½“å‰æ–‡ä»¶ä¸­çš„æœ‰æ•ˆè¡Œåˆ—è¡¨ä¸åŒï¼Œåˆ™æ›´æ–°
        if unique_urls != current_file_ideal_lines:
            print(f"ğŸ”„ Updating {filename}: Normalizing URLs, removing duplicates/invalid entries...")
            try:
                with open(filename, 'w', encoding='utf-8') as file:
                    file.write('\n'.join(unique_urls) + '\n') #ç¡®ä¿æœ«å°¾æœ‰æ¢è¡Œ
                print(f"âœ… {filename} has been updated successfully with {len(unique_urls)} URLs.")
            except IOError as e:
                print(f"âŒ Error writing updates to {filename}: {e}")
                return initial_urls # è¿”å›åŸå§‹URLsï¼Œå› ä¸ºå†™å…¥å¤±è´¥
        else:
            print(f"âœ… No structural changes needed for {filename}. Already clean.")
    else:
        print(f"âœ… No changes needed for {filename}. Already clean.")
    return unique_urls


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(f"Usage: python {os.path.basename(__file__)} <url_filename>")
        sys.exit(1)

    url_source_filename = sys.argv[1]

    # 1ï¸âƒ£ å»é‡ã€å»ç©ºè¡Œå¹¶è§„èŒƒ URL
    print(f"--- Step 1: Processing URL file: {url_source_filename} ---")
    urls_to_check = process_url_file(url_source_filename)

    if not urls_to_check:
        print(f"âš ï¸ No valid URLs to check in {url_source_filename}. Exiting.")
        sys.exit(1)
    print(f"Found {len(urls_to_check)} unique and valid URLs to monitor.")

    # 2ï¸âƒ£ å¹¶å‘æ£€æŸ¥æ¯ä¸ªç½‘ç«™çš„çŠ¶æ€
    print(f"\n--- Step 2: Checking website statuses (max_workers={MAX_WORKERS}) ---")
    all_results = [] # å­˜å‚¨æ‰€æœ‰ç»“æœï¼ŒåŒ…æ‹¬é”™è¯¯
    
    # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘è¯·æ±‚
    # ä¸ºäº†ä¿æŒREADMEä¸­URLçš„é¡ºåºä¸è¾“å…¥æ–‡ä»¶ä¸€è‡´ï¼Œå…ˆè·å–ç»“æœï¼Œå†æŒ‰åŸé¡ºåºæ•´ç†
    results_map = {} # ä½¿ç”¨å­—å…¸ä»¥URLä¸ºé”®å­˜å‚¨ç»“æœï¼Œæ–¹ä¾¿åç»­æŒ‰åŸé¡ºåºæ’åº
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(check_website_status, url): url for url in urls_to_check}
        for future in as_completed(future_to_url):
            original_url = future_to_url[future]
            try:
                result = future.get()
                results_map[original_url] = result
            except Exception as exc:
                print(f"âŒ URL {original_url} generated an exception during threaded execution: {exc}")
                # åˆ›å»ºä¸€ä¸ªé”™è¯¯ç»“æœæ¡ç›®
                error_result = {
                    "url": original_url, "status": "âŒ Error (Thread Exception)", "status_code": None,
                    "response_time": "N/A", "timestamp": datetime.now(timezone.utc).isoformat(), "error": str(exc)
                }
                results_map[original_url] = error_result
    
    # æŒ‰ urls_to_check çš„åŸå§‹é¡ºåºæ•´ç†ç»“æœ
    ordered_results = [results_map[url] for url in urls_to_check if url in results_map]


    # 3ï¸âƒ£ æ›´æ–° README.md
    print(f"\n--- Step 3: Updating {README_FILENAME} ---")
    update_readme(ordered_results, readme_file=README_FILENAME)

    # 4ï¸âƒ£ ç­›é€‰å‡ºé404çš„URLsï¼Œå¹¶æ›´æ–°åŸå§‹URLæ–‡ä»¶
    print(f"\n--- Step 4: Updating URL file {url_source_filename} by removing 404s ---")
    valid_urls_after_check = []
    removed_404_count = 0
    for result in ordered_results: # ä½¿ç”¨æœ‰åºçš„ç»“æœæ¥å†³å®šå“ªäº›URLä¿ç•™
        # ä¿ç•™é404çš„ï¼Œæˆ–è€…é‚£äº›è™½ç„¶å‡ºé”™ä½†ä¸æ˜¯å› ä¸º404ï¼ˆä¾‹å¦‚è¶…æ—¶ã€è¿æ¥é”™è¯¯ï¼‰
        # å¦‚æœè¦ä¸¥æ ¼åªä¿ç•™æˆåŠŸçš„ï¼Œå¯ä»¥è°ƒæ•´æ¡ä»¶ä¸º result.get('status_code') and 200 <= result.get('status_code') < 300
        if result.get('status_code') != 404 :
             # å¦‚æœURLé‡å®šå‘äº†ï¼Œæˆ‘ä»¬åº”è¯¥ä¿ç•™åŸå§‹çš„ã€ç”¨æˆ·æä¾›çš„URLï¼ˆå¦‚æœå®ƒæ˜¯åˆ—è¡¨çš„ä¸€éƒ¨åˆ†ï¼‰
             # result['url'] å§‹ç»ˆæ˜¯åŸå§‹è¯·æ±‚çš„URL
            valid_urls_after_check.append(result['url'])
        else:
            print(f"ğŸ—‘ï¸ Marking {result['url']} for removal from {url_source_filename} due to 404 status.")
            removed_404_count +=1
    
    # åªæœ‰å½“å­˜æ´»çš„URLåˆ—è¡¨ä¸æ£€æŸ¥å‰çš„URLåˆ—è¡¨ä¸åŒæ—¶æ‰é‡å†™æ–‡ä»¶
    # ï¼ˆå³ï¼Œç¡®å®æœ‰404çš„URLè¢«ç§»é™¤äº†ï¼‰
    if removed_404_count > 0:
        print(f"ğŸ”„ Updating {url_source_filename}: Removing {removed_404_count} URLs with 404 status.")
        try:
            with open(url_source_filename, 'w', encoding='utf-8') as file:
                file.write('\n'.join(valid_urls_after_check) + '\n') #ç¡®ä¿æœ«å°¾æœ‰æ¢è¡Œ
            print(f"âœ… {url_source_filename} updated. Now contains {len(valid_urls_after_check)} URLs.")
        except IOError as e:
            print(f"âŒ Error writing updated URL list to {url_source_filename}: {e}")
    else:
        print(f"âœ… No URLs with 404 status found to remove from {url_source_filename}. No changes made to URL list.")
    
    print("\nç›‘æµ‹è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
